{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eede2d84",
   "metadata": {},
   "source": [
    "# POLI 179 Final Project\n",
    "## LDA of the entire corpus \n",
    "### By: Alyson OtaÃ±ez "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c669180",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dfa31",
   "metadata": {},
   "source": [
    "The following code applies an LDA model to the entire `ie_cities.csv` file found in the `Data` folder.\n",
    "\n",
    "Topic Plot can be found in the folders - `Plots` -> `LDA_Topic_Visual` -> `Corpus`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7cdbb",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab19822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if necessary\n",
    "# ! pip install nltk\n",
    "# ! pip install spacy \n",
    "# ! pip install --user gensim\n",
    "# ! pip install --user pyLDAvis\n",
    "# ! pip install --user gutenbergpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "sys.path.append('/home/aotanez/.local/lib/python3.9/site-packages') # Comment out\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gutenbergpy import textget\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvisualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "ie_cities = pd.read_csv('../Data/ie_cities.csv')\n",
    "\n",
    "# Drop NA values (only 1)\n",
    "ie_cities = ie_cities[ie_cities['Text'].notna()]\n",
    "\n",
    "ie_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f930e71",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet for lemmatization \n",
    "def wordnet_pos_tags(x):\n",
    "    if x.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif x.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif x.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif x.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing \n",
    "def txt_preprocess_pipeline(text):\n",
    "    standard_txt = text.lower()\n",
    "    \n",
    "    clean_txt = re.sub(r'http\\S+|www\\S+|https\\S+', '', standard_txt, flags = re.MULTILINE)\n",
    "    clean_txt = re.sub(r'\\n', ' ', clean_txt)\n",
    "    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n",
    "    clean_txt = re.sub(r'\\S+@\\S+', '', clean_txt)\n",
    "    clean_txt = re.sub(r'\\\\r\\\\n', ' ', clean_txt)\n",
    "    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n",
    "    clean_txt = re.sub(r'<.*?>', '', clean_txt)\n",
    "    clean_txt = re.sub(r'[^\\w\\s]', '', clean_txt)    \n",
    "    clean_txt = re.sub(r'\\b\\w{1,2}\\b', '', clean_txt)\n",
    "    \n",
    "    tokens = word_tokenize(clean_txt)\n",
    "    filtered_tokens_alpha = [word for word in tokens if word.isalpha() and not re.match(r'^[ivxlcdm]+$', word)]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['chino', 'fontana', 'march', 'joint', 'powers', 'authority', \n",
    "                       'http', 'rialto', 'ontario', 'city', 'council', 'agenda',\n",
    "                      'meeting', 'minutes', 'back', 'site', 'main', 'welcome', 'browse', 'video',\n",
    "                      'monday', 'tuesday', 'wednesday', 'thursday', 'friday', \n",
    "                      'saturday', 'sunday', 'notice', 'commission', 'archive', 'pmcity',\n",
    "                      'chamber', 'palm', 'ave', 'january', 'february', 'march', 'april', 'may',\n",
    "                      'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
    "                      'closed', 'session'])\n",
    "    filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens_final)\n",
    "    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n",
    "    \n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704cbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply functions to data\n",
    "ie_cities['Processed_Text'] = ie_cities['Text'].apply(txt_preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf98841",
   "metadata": {},
   "source": [
    "### 3. Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "dictionary = corpora.Dictionary(ie_cities['Processed_Text'])\n",
    "dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "# Generate corpus as BoW\n",
    "corpus = [dictionary.doc2bow(i) for i in  ie_cities['Processed_Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80203f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus = corpus, id2word = dictionary, random_state = 4583, \n",
    "                     chunksize = 20, num_topics = 7, passes = 200, iterations= 400)\n",
    "\n",
    "# Print LDA topics\n",
    "for idx, topic in lda_model.print_topics(num_topics = 7, num_words =10):\n",
    "    print(f\"Topic {idx+1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488156b",
   "metadata": {},
   "source": [
    "### 4. Plot Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "dickens_visual = gensimvisualize.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "pyLDAvis.save_html(dickens_visual, 'lda_corpus_visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "pyLDAvis.display(dickens_visual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
