{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f595a6c7",
   "metadata": {},
   "source": [
    "# POLI 179 Final Project\n",
    "### By: Alyson Ota√±ez "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b40fc4",
   "metadata": {},
   "source": [
    "## Scraping city council agenda text for: Ontario, Fontana, Rialto, and March Joint Powers Authority "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d5c1f",
   "metadata": {},
   "source": [
    "### Website Links:\n",
    "* Ontario - https://www.ontarioca.gov/Agendas/CityCouncil\n",
    "* Fontana - https://fontana.legistar.com/Calendar.aspx\n",
    "* Rialto - https://rialto.legistar.com/Calendar.aspx\n",
    "* March Joint Powers Authority - https://marchjpa.com/meetings-agendas/\n",
    "* Chino - http://chinocityca.iqm2.com/Citizens/Calendar.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ea125",
   "metadata": {},
   "source": [
    "#### Note: This code only reads the text for pdf's, some pdf's were scanned, meaning they were images, this text was read using R. This extra step was needed for: Ontario and the March Joint Powers Authority. The entire text of the city of Chino was extracted using R, date and year was extracted using python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104c74d",
   "metadata": {},
   "source": [
    "#### Note: This code should be ran separately for each website. The code below is just a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f79052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages, may need to use 'pip install' before importing\n",
    "\n",
    "import codecs\n",
    "from PyPDF2 import PdfReader\n",
    "import webbrowser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read html's and save contents \n",
    "html = [] # Download html contents and store them in your working directory, read them in here as strings \n",
    "          # Will need to download all pages if the contents span more than one page\n",
    "\n",
    "full_html = '' # Empty string to store contents of html's\n",
    "\n",
    "for filename in html: # For loop to open and read the contents of each html file\n",
    "    with codecs.open(filename, 'r') as f:\n",
    "        full_html += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the keyword that corresponds to all agenda links \n",
    "\n",
    "keyword = '' # Agendas have a common name. \n",
    "             # Example: all links contain the phrase 'agenda-file' that would be the keyword\n",
    "count = 0\n",
    "found = [0] * len(full_html)\n",
    "for i in range(len(full_html)):\n",
    "    if full_html[i:i+len(keyword)] == keyword:\n",
    "        found[count] = i\n",
    "        count += 1\n",
    "        \n",
    "found = found[0:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the agenda links \n",
    "\n",
    "file_links = [] # Empty list to store links \n",
    "\n",
    "for i in range(len(found)): # For loop to extract links based on where they were \"found\" above\n",
    "    index = found[i]\n",
    "    while(full_html[index] != '\\\"'):\n",
    "        index += 1\n",
    "    file_links = file_links + [full_html[found[i]:index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the base url to complete the links\n",
    "\n",
    "complete_links = [] # Empty list to store complete links \n",
    "base_url = \"https://www.ontarioca.gov/\" # Example\n",
    "for link in file_links:\n",
    "    complete_links.append(base_url + link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c910250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download pdfs based on the links in complete_links and extract text \n",
    "\n",
    "def download_pdf(url, max_retries=3): # Downloads pdf temporarily within the directory \n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.content\n",
    "        except ChunkedEncodingError: # Code proceeds if there is still an error after the 3rd attempt \n",
    "            print(\"Connection error occurred. Retrying after 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(url):\n",
    "    pdf_data = download_pdf(url) # Downloads pdf using function above\n",
    "    if pdf_data is None:\n",
    "        print(f\"Failed to download PDF from URL: {url}\")\n",
    "        return ''\n",
    "\n",
    "    with open('temp.pdf', 'wb') as f: # Opens pdf\n",
    "        f.write(pdf_data)\n",
    "\n",
    "    with open('temp.pdf', 'rb') as f: # Extracts text from each page, reading each pdf page together\n",
    "        reader = PdfReader(f)\n",
    "        text = ''\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text += page.extract_text()\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting text from page {page_num + 1}: {e}\") # Code proceeds if there is an error \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function the complete_links \n",
    "\n",
    "df = pd.DataFrame(columns=['PDF Link', 'Text']) # Empty dataframe to store data \n",
    "\n",
    "for link in complete_links:\n",
    "    try:\n",
    "        text = extract_text_from_pdf(link)\n",
    "        df = df.append({'PDF Link': link, 'Text': text}, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF link: {link}. Exception: {e}\") # Code proceeds if there is an error \n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a9e21",
   "metadata": {},
   "source": [
    "# Next step is initially for Fontana and Rialto \n",
    "### March Joint Powers and Ontario had missing text given that some of their PDFs were scanned. This text was then extracted in R. The data was then read back in python where the functions below were applied to get the date and year. Dates and year for the city of Chino were read using python, text was gathered in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the date from the text\n",
    "\n",
    "date_regex = r\"(Sunday|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday), (January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\"\n",
    "\n",
    "def extract_date(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(date_regex, text)\n",
    "        if match:\n",
    "            return match.group(0) # Returns date in Day, Month, Year format\n",
    "    return None\n",
    "\n",
    "# Apply function to df\n",
    "df['Date'] = df['Text'].apply(extract_date)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efed533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the year\n",
    "\n",
    "date_regex = r\"(Sunday|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday), (January|February|March|April|May|June|July|August|September|October|November|December) (\\d{1,2}), (\\d{4})\"\n",
    "\n",
    "def extract_year(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(date_regex, text)\n",
    "        if match:\n",
    "            return match.group(4) # Returns the year only \n",
    "    return None\n",
    "\n",
    "df['Year'] = df['Text'].apply(extract_year)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as csv \n",
    "\n",
    "df.to_csv('df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
